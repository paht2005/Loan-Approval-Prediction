# ğŸ¦ CS116 Project: Loan Approval Prediction 

[![Kaggle Competition](https://img.shields.io/badge/Kaggle-Playground_S4E10-blue)](https://www.kaggle.com/competitions/playground-series-s4e10)

> This repository contains the full implementation of a machine learning pipeline developed for the Kaggle competition **Loan Approval Prediction (Playground Series - Season 4, Episode 10)**. Our goal is to predict whether a loan will be approved based on applicant information and credit history, leveraging state-of-the-art ML models and careful feature engineering.

---
# Team Information
| No.    | Student ID      | Full Name in Vietnamese        | Position   | Github                                       | Email                   |
| ------ |:---------------:| ------------------------------:|-----------:|---------------------------------------------:|-------------------------:
| 1      | 23521570        | Huá»³nh Viá»‡t Tiáº¿n                |Leader      |[SharkTien](https://encr.pw/SCu2w)            |23521570@gm.uit.edu.vn   |
| 2      | 23521143        | Nguyá»…n CÃ´ng PhÃ¡t               |Member      |[paht2005](https://github.com/paht2005)       |23521143@gm.uit.edu.vn   |
| 3      | 23520123        | Nguyá»…n Minh Báº£o                |Member      |      |23520123@gm.uit.edu.vn   |        
| 4      | 23520133        | Pháº¡m PhÃº Báº£o                   |Member      |[itsdabao](https://github.com/itsdabao)       |23520133@gm.uit.edu.vn   |

## ğŸ“– Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ—‚ï¸ Repository Structure](#ï¸-repository-structure)
- [ğŸš€ Pipeline Overview](#-pipeline-overview)
  - [1. Data Preprocessing](#1-data-preprocessing)
  - [2. Model Training](#2-model-training)
  - [3. Feature Selection](#3-feature-selection)
  - [4. Evaluation](#4-evaluation)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸ¯ Usage](#-usage)
- [ğŸ“ˆ Results](#-results)
- [ğŸ“Œ Conclusion](#-conclusion)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## âœ¨ Features

- Full notebook implementation for EDA, preprocessing, training, evaluation.
- Comparison between multiple models: **XGBoost, LightGBM, CatBoost**.
- Automatic encoding and feature dropping based on correlation and performance.
- Uses **F1 Macro Score** as primary evaluation metric.
- Performance was evaluated using a single 80/20 trainâ€“test split instead of cross-validation.

---

## ğŸ—‚ï¸ Repository Structure

```
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ train.csv
â”‚ â”œâ”€â”€ test.csv
â”‚ â””â”€â”€ sample_submission.csv
â”œâ”€â”€ CS116Project_LoanApprovalPrediciton.ipynb # Main Jupyter notebook
â”œâ”€â”€ requirements.txt # Python Dependencies
â””â”€â”€ README.md # Project overview

```

---

## ğŸš€ Pipeline Overview

### 1. Data Preprocessing

- Handling missing values (median, mode).
- Label encoding for categorical features.
- Outlier removal on numeric columns.
- Correlation matrix for feature insights.

### 2. Model Training

- Applied and compared:
  - **XGBoost**
  - **LightGBM**
  - **CatBoost**
- Used train_test_split (80/20) for model evaluation and comparison.
- Metrics:
  - Accuracy
  - F1 Macro

### 3. Feature Selection

- Dropped low-value features based on:
  - Correlation matrix
  - Model interpretability
- Final dropped features:
  - `cb_person_cred_hist_length`
  - `cb_person_default_on_file_encoded`

### 4. Evaluation

- Evaluated each model before and after feature selection.
- Highlighted performance boosts post feature removal.

---

## âš™ï¸ Installation
1. **Clone the repository**:
   ```bash
   git clone https://github.com/paht2005/Loan-Approval-Prediction.git
   cd Loan-Approval-Prediction
   ```
2. **(optional) Create environment:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   pip install -r requirements.txt
   ```
3. **Open the notebook:**
   ```bash
   jupyter notebook CS116Project_LoanApprovalPrediciton.ipynb
   ```
## ğŸ“ˆ Results
```
| Model    | All Features | After Dropping 2 Features |
| -------- | ------------ | ------------------------- |
| XGBoost  | 94.90%       | 95.83%                    |
| LightGBM | 94.67%       | 94.83%                    |
| CatBoost | 94.70%       | 94.87%                    |
```
## ğŸ“Œ Conclusion
After performing model comparisons and testing with feature elimination, we found that removing ``cb_person_cred_hist_length`` and ``cb_person_default_on_file_encoded`` led to consistent performance improvements across all models.

Among them, **XGBoost** achieved the **best result** with:
- **F1 Macro Score: 95.83%**
- **After dropping 2 low-impact features**

This confirms that **careful feature selection** can have significant positive effects on model performanc

## ğŸ¤ Contributing
Contributions, improvements, or issue reports are welcome. Please open a pull request or submit an issue!

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.

